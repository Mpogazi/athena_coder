{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMuqHhVHeQfjyRqDAo8PUaC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mpogazi/athena_coder/blob/main/notebooks/video_%26%26_sound.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Video and Sound GPT (Generation of Videos)\n",
        "The main idea in this notebook is to use the gpt model architecture or another different architecture (JEPA for example) to produce plausible videos.\n",
        "\n",
        "### Theory/Intuition:\n",
        "I think it's possible to learn the pixel distributions of the in the time dimension and predict the next frame in videos and other modalities as it has been done for texts. (GPT 3 or GPT 4)\n",
        "\n",
        "### Initial Approach\n",
        "* download hundreds of videos from YouTube and put them on Google drive\n",
        "* Write code to read the videos and transform them into Tensors\n",
        "* Write code to transform or read an output tensors into video format\n",
        "* Write a window code to play the generated video vis as vis the original video (A bit tricky, will add more details as time goes.)"
      ],
      "metadata": {
        "id": "4GRWwksKXXsV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gdk-xH4Yfz-5",
        "outputId": "d5b6510e-5ba5-4661-dc0e-d15dcf9b3ae2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (0.25.1)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.23.5)\n"
          ]
        }
      ],
      "source": [
        "# Installing the python libraries to handle reading sound and video\n",
        "!pip install opencv-python pydub"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import\n",
        "\n",
        "Please put all the imports here. We would like to have a single source of truth."
      ],
      "metadata": {
        "id": "ABx-XBVxD6fu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from pydub import AudioSegment\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import io\n",
        "import os\n",
        "\n",
        "# Google drive imports\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "pUeR8aAohh8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Globals\n",
        "In this section we mount the memory (Drive) and set up some global variables.\n",
        "List of globals:\n",
        "\n",
        "`base_path`, `MAX_HEIGHT`, `MAX_WIDTH`"
      ],
      "metadata": {
        "id": "Z9W-6o-uC5t6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting the drive with the content\n",
        "# Might need to give permission on this.\n",
        "# Since someone needs to access the contents of the drive/videos\n",
        "drive.mount('/content/drive')\n",
        "BASE_PATH = '/content/drive/MyDrive/video_model/'\n",
        "MAX_HEIGHT = 720\n",
        "MAX_WIDTH = 1280\n",
        "TRAIN_SPLIT = 0.9\n",
        "BATCH_SIZE = 32"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6I6pX8xhpvy",
        "outputId": "af86e203-f616-4233-af3b-3f8b59419724"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting Up The Training Data\n",
        "\n",
        "1. Create a list of all files\n",
        "2. load the videos to set up some Global Variables"
      ],
      "metadata": {
        "id": "ash84Lf2DniT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files = set()\n",
        "for filename in os.listdir(BASE_PATH):\n",
        "  files.add(filename)\n",
        "\n",
        "def split_test(files):\n",
        "  size = int(len(files) * TRAIN_SPLIT)\n",
        "  input_list = list(files)\n",
        "  random.shuffle(input_list)\n",
        "  return input_list[:size], input_list[size:]\n",
        "\n",
        "train, val = split_test(files)"
      ],
      "metadata": {
        "id": "VLMRPbhBzssV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Training Data\n",
        "\n",
        "1. Load the videos and pad them\n",
        "2. Collapse all the videos into a single giant data object\n",
        "3. Split the vidoes into train and test data"
      ],
      "metadata": {
        "id": "NXzHXkXyEHuX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We will need to pad the tensors to handle MAX_WIDTH and MAX_HEIGHT\n",
        "# Frames are gonna be the same\n",
        "def pad_video_tensor(video_tensor):\n",
        "  # (frames, height, width, chanel)\n",
        "  _, height, width, _ = tf.shape(video_tensor).numpy()\n",
        "  paddings = tf.constant([\n",
        "      [0, 0],\n",
        "      [0, MAX_HEIGHT - height],\n",
        "      [0, MAX_WIDTH - width],\n",
        "      [0, 0]\n",
        "  ])\n",
        "  return tf.pad(video_tensor, paddings, \"CONSTANT\", constant_values=0)\n",
        "\n",
        "\n",
        "# take a video path and return an equivalent tensor\n",
        "# Returns tensor of shape (Frames, MAX_HEIGHT, MAX_WIDTH, 3)\n",
        "def capture_frames_randomly(video_path, frame_limits = 200):\n",
        "  cap = cv2.VideoCapture(video_path)\n",
        "  video_tensors = []\n",
        "\n",
        "  frame_index = 0\n",
        "  limit = frame_limits\n",
        "  start = random.randint(0, cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "  cap.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
        "  for i in range(start, start + limit + 1):\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "      print(\"Error: Failed to grab frame.\")\n",
        "      break\n",
        "    video_tensors.append(tf.convert_to_tensor(frame, dtype=tf.int16))\n",
        "  cap.release()\n",
        "\n",
        "  ## Padding the tensor to match the sizes for all videos\n",
        "  return pad_video_tensor(tf.convert_to_tensor(video_tensors, dtype=tf.int16))"
      ],
      "metadata": {
        "id": "yyU9l3qQh5Iu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Batching\n",
        "Since the video sizes are humongous, we need to implement a\n",
        "batching strategy that is not conventional.\n",
        "\n",
        "We will be picking a file by random and randomly pick a batch.\n",
        "\n",
        "Some Math:\n",
        "We 're working in milli seconds. Therefore, we will be picking\n",
        "this much time in a video randomly (batch_size * (1 / 25) fps)\n",
        "\n",
        "batch_size is counted in frames.\n"
      ],
      "metadata": {
        "id": "0wGh3JO5DeHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split: str):\n",
        "  data = train if split == 'train' else val\n",
        "  rand_file = data[random.randint(0, len(data))]\n",
        "  # Since in our predictions we're taking in the past and predicting the future\n",
        "  sample = capture_frames_randomly(BASE_PATH + rand_file, BATCH_SIZE)\n",
        "  x = sample[:BATCH_SIZE]\n",
        "  y = sample[1:(BATCH_SIZE + 1)]\n",
        "  return x, y\n",
        "\n",
        "xb, yb = get_batch('train')"
      ],
      "metadata": {
        "id": "KUh7KSWWlgY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"shape xb: \", tf.shape(xb))\n",
        "print(\"shape yb: \", tf.shape(yb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6viKq6h3Lzb0",
        "outputId": "ef6f1cbd-3fef-4f0b-c4cc-4be85f5e6914"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape xb:  tf.Tensor([  32  720 1280    3], shape=(4,), dtype=int32)\n",
            "shape yb:  tf.Tensor([  32  720 1280    3], shape=(4,), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modeling\n",
        "\n",
        "This is the modeling part of the notebook. Watch as we create the model lmaooo!"
      ],
      "metadata": {
        "id": "G07F_rbZQf-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTVideoModel(keras.Model):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def call(self, idx, targets=None):\n",
        "\n",
        "  def generate(self, idx, max_new_frames):\n",
        "    return idx\n",
        "\n",
        "m = GPTVideoModel()\n",
        "out, loss = m(xb, yb)\n",
        "optimizer = keras.optimizers.AdamW(learning_rate=learning_rate)"
      ],
      "metadata": {
        "id": "y7BE4M5eQpnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7KDy5toCRe_a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}