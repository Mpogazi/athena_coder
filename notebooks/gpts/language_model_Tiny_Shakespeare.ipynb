{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOjM1dW2166uv+UEz3lOrG6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mpogazi/athena_coder/blob/main/language_model_Tiny_Shakespeare.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Language Model - Tiny Shakespeare\n",
        "\n",
        "This notebook is an implementation of character-based GPT model, trained on tiny shakespeare. The code is written in tensorflow, but it's based on this [Andrej's video](https://www.youtube.com/watch?v=kCc8FmEb1nY).\n",
        "\n",
        "Suggested improvements to learn more should:\n",
        "* Train the model to add numbers\n",
        "* Train the model to be a calculator\n",
        "* Train the model to generate code\n",
        "* Train the model on a different language dataset\n",
        "* Implement additional transformer papers"
      ],
      "metadata": {
        "id": "4XPuXBMWnsGH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhtVZc1xpH9B"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import io"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mounting the google drive to the notebook\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upR_-srUpULS",
        "outputId": "56a75f90-f983-4c98-f42c-1779a63a3d8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the file in as text\n",
        "with open('/content/drive/MyDrive/language_model/tiny-shakespeare.txt', 'r') as infile:\n",
        "  text = infile.read()"
      ],
      "metadata": {
        "id": "oLRq5Dw8pbuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of the dataset in character: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UO9CtZ6gpwj5",
        "outputId": "0e9b9513-1c15-444a-88b4-c3b6f293515a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of the dataset in character:  1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(\"\".join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95vQwwP_thei",
        "outputId": "a87a1baf-09ab-4e00-b80f-bdde826a8149"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "tf.random.set_seed(1337)\n",
        "# if device_name != '/device:GPU:0':\n",
        "#   raise SystemError(\"GPU device not found\")\n",
        "# print('Found GPU at: {}'.format(device_name))\n",
        "batch_size = 64\n",
        "block_size = 256\n",
        "n_embed = 384\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "eval_iters = 10 #200\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2"
      ],
      "metadata": {
        "id": "FC65kxTfM-76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stoi = {ch:i for i, ch in enumerate(chars)}\n",
        "itos = {i:ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])"
      ],
      "metadata": {
        "id": "8MBZqH8cthuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = tf.convert_to_tensor(encode(text), dtype=tf.int32)"
      ],
      "metadata": {
        "id": "neq37xD-tx39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "Y9z9qa26tylh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split: str):\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = tf.random.uniform((batch_size,), maxval=len(data) - block_size, dtype=tf.int32)\n",
        "  x = tf.stack([data[i:i+block_size] for i in ix])\n",
        "  y = tf.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  return x, y\n",
        "\n",
        "xb, yb = get_batch('train')"
      ],
      "metadata": {
        "id": "9uny3DFDty8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(layers.Layer):\n",
        "  def __init__(self, n_head, head_size):\n",
        "    super().__init__()\n",
        "    self.c_attn = layers.Dense(n_embed * 3)\n",
        "\n",
        "    #self.heads = [Head(head_size) for _ in range(num_heads)]\n",
        "    self.proj  = layers.Dense(n_embed)\n",
        "    self.dropout = layers.Dropout(dropout)\n",
        "\n",
        "  def call(self, x):\n",
        "    B, T, C = x.shape\n",
        "    k, q, v = tf.split(self.c_attn(x), num_or_size_splits=3, axis=-1)\n",
        "\n",
        "    # reshape from (B, T, n_embed) to (B, nh, T, hs), Note: nh: n_head, hs: head_size\n",
        "    k = tf.transpose(tf.reshape(k, [B, T, n_head, n_embed // n_head]), perm=[0, 2, 1, 3])\n",
        "    q = tf.transpose(tf.reshape(q, [B, T, n_head, n_embed // n_head]), perm=[0, 2, 1, 3])\n",
        "    v = tf.transpose(tf.reshape(v, [B, T, n_head, n_embed // n_head]), perm=[0, 2, 1, 3])\n",
        "\n",
        "    # lower triangular mask\n",
        "    tril = tf.linalg.band_part(tf.ones((T, T)), -1, 0) # (T, T)\n",
        "    tril = tf.reshape(tril, [1, 1, T, T]) # (1, 1, T, T)\n",
        "\n",
        "    # (B, nh, T, hs) @ (B, nh, hs, T) -> (B, nh, T, T)\n",
        "    att = (q @ tf.transpose(k, perm=[0, 1, 3, 2])) * (k.shape[-1]**-0.5)\n",
        "    att = tf.where(tril[:,:,:T,:T] == 0, float('-inf'), att)\n",
        "    att = tf.nn.softmax(att)\n",
        "\n",
        "    y = att @ v # (B, nh, T, T) @ (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "    y = tf.reshape(tf.transpose(y, perm=[0, 2, 1, 3]), [B, T, n_embed])\n",
        "\n",
        "    y = self.dropout(self.proj(y))\n",
        "    return y\n",
        "\n",
        "class FeedForward(layers.Layer):\n",
        "  def __init__(self, n_embed):\n",
        "    super().__init__()\n",
        "    self.network = keras.Sequential([\n",
        "        layers.Dense(4 * n_embed, activation=\"relu\", name=\"feedforward\"),\n",
        "        layers.Dense(n_embed),\n",
        "        layers.Dropout(dropout)\n",
        "    ])\n",
        "\n",
        "  def call(self, x):\n",
        "    return self.network(x)\n",
        "\n",
        "class Block(layers.Layer):\n",
        "  def __init__(self, n_embed, n_head):\n",
        "    super().__init__()\n",
        "    head_size = n_embed // n_head\n",
        "    self.sa = MultiHeadAttention(n_head, head_size)\n",
        "    self.ffwd = FeedForward(n_embed)\n",
        "\n",
        "    self.ln1 = layers.LayerNormalization()\n",
        "    self.ln2 = layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x):\n",
        "    x = x + self.sa(self.ln1(x))\n",
        "    x = x + self.ffwd(self.ln2(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "BIkBb7yYl_xK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LanguageModel(keras.Model):\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = keras.layers.Embedding(vocab_size, n_embed)\n",
        "    self.position_embedding_table = layers.Embedding(block_size, n_embed)\n",
        "\n",
        "    self.blocks = keras.Sequential([Block(n_embed, n_head) for _ in range(n_layer)])\n",
        "    self.ln_f = layers.LayerNormalization()\n",
        "    self.ffwd = FeedForward(n_embed)\n",
        "\n",
        "    self.lm_head = layers.Dense(vocab_size)\n",
        "    self.loss_calc = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "  def call(self, idx, targets=None):\n",
        "    B, T = idx.shape\n",
        "    # idx and targets are both (B, T) tensor of integers\n",
        "    tok_embed = self.token_embedding_table(idx) # (B, T, C)\n",
        "    pos_embed = self.position_embedding_table(tf.range(T))\n",
        "\n",
        "    x = tok_embed + pos_embed\n",
        "    x = self.blocks(x)\n",
        "    x = self.ln_f(x)\n",
        "    x = self.ffwd(x)\n",
        "\n",
        "    logits = self.lm_head(x)\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      loss = self.loss_calc(targets, logits)\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    # idx (B, T)\n",
        "    for _ in range(max_new_tokens):\n",
        "      # logits are (B, T, C)\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "      logits, _ = self(idx_cond)\n",
        "      logits = logits[:, -1,:] # (B, C)\n",
        "      probs = tf.nn.softmax(logits) # (B, C)\n",
        "      idx_next = tf.random.categorical(logits, 1) # (B, 1)\n",
        "      idx = tf.concat([idx, idx_next], 1) # (B, T + 1)\n",
        "    return idx\n",
        "\n",
        "m = LanguageModel(vocab_size)\n",
        "out, loss = m(xb, yb)\n",
        "optimizer = keras.optimizers.AdamW( learning_rate=learning_rate)"
      ],
      "metadata": {
        "id": "w7OWGoUkth_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3XihFi9iVVz",
        "outputId": "b02f3475-ecc4-4462-ab33-9869fb0e58b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"language_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  24960     \n",
            "                                                                 \n",
            " embedding_1 (Embedding)     multiple                  98304     \n",
            "                                                                 \n",
            " sequential_6 (Sequential)   (64, 256, 384)            10646784  \n",
            "                                                                 \n",
            " layer_normalization_12 (Lay  multiple                 768       \n",
            " erNormalization)                                                \n",
            "                                                                 \n",
            " feed_forward_6 (FeedForward  multiple                 1181568   \n",
            " )                                                               \n",
            "                                                                 \n",
            " dense_19 (Dense)            multiple                  25025     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11,977,409\n",
            "Trainable params: 11,977,409\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(x, y, model, optimizer):\n",
        "  with tf.GradientTape() as tape:\n",
        "    logits, loss = m(x, y)\n",
        "\n",
        "  gradients = tape.gradient(loss, m.trainable_variables)\n",
        "  #print(gradients)\n",
        "  optimizer.apply_gradients(zip(gradients, m.trainable_variables))\n",
        "  return loss\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "  if iter % eval_interval == 0:\n",
        "    out = {}\n",
        "    for split in ['train', 'val']:\n",
        "      losses = [None] * eval_iters\n",
        "      for k in range(eval_iters):\n",
        "        x, y = get_batch(split)\n",
        "        logits, loss = m(x, y)\n",
        "        losses [k] = loss.numpy()\n",
        "      mean_loss = tf.reduce_mean(losses)\n",
        "      out[split] = mean_loss.numpy()\n",
        "\n",
        "    print(f\"step {iter}: train loss {out['train']:.4f}, val loss {out['val']:.4f}\")\n",
        "\n",
        "  x, y = get_batch('train')\n",
        "  loss = train_step(x, y, m, optimizer)\n",
        "\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TT6oRsOSBzMN",
        "outputId": "bb99f86d-971c-40bd-aff6-269765c3612d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 3.4237, val loss 3.4524\n",
            "step 500: train loss 1.8440, val loss 1.9693\n",
            "step 1000: train loss 1.4227, val loss 1.6350\n",
            "step 1500: train loss 1.2694, val loss 1.5867\n",
            "step 2000: train loss 1.1694, val loss 1.5983\n",
            "step 2500: train loss 1.0497, val loss 1.6617\n",
            "step 3000: train loss 0.9223, val loss 1.7734\n",
            "step 3500: train loss 0.7548, val loss 2.0067\n",
            "step 4000: train loss 0.6070, val loss 2.2829\n",
            "step 4500: train loss 0.4562, val loss 2.6583\n",
            "tf.Tensor(0.36114192, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(tf.zeros((1, 1), dtype=tf.int64), max_new_tokens=1000)[0].numpy().tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRcIryBsGNlz",
        "outputId": "8c31ab94-f98a-4d8a-8635-3b9bc0154ca9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ne'er shall be thrippeten whispering flower\n",
            "Post:\n",
            "But made he had eaten to him earn'd with these dog!\n",
            "Dared, even ranks may be matter to the Tower,\n",
            "And we to can with our love is false, that fear\n",
            "All that wouldst live: do not rejoin'd,\n",
            "We should have answer to extumpe this hath dewry.\n",
            "I must be traitor's unto the crown he look:\n",
            "I prithee, look up: if you can breath and the\n",
            "duke him, this be advertised him. I would prick again\n",
            "to Marcius. I never so bless have been more, mighty\n",
            "ones and much lientent.\n",
            "Lords, Edward, noble for anger, and thy moody is\n",
            "therefore, if thoughts fear'd blood. Come on, I pray;\n",
            "It is a proness to the clouds and rub are rised,\n",
            "and accused him out only that my name in new--\n",
            "And she hath fly me; indea--\n",
            "Her made my lord, hath still dines my life\n",
            "In Longer London on rescued a father.\n",
            "\n",
            "MARIUS:\n",
            "Is't true able upon'd.\n",
            "\n",
            "First Citizen:\n",
            "The people and sweet of advice\n",
            "Scanding curses vial. You gave me presently to you\n",
            "For your bestooms, and she is any yet all.\n",
            "\n",
            "Second Citi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIrWNcWmC0so"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/language_model/sample_generation.txt', 'w') as f:\n",
        "  f.write(decode(m.generate(tf.zeros((1, 1), dtype=tf.int64), max_new_tokens=10000)[0].numpy().tolist()))"
      ]
    }
  ]
}