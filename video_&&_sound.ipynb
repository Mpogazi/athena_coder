{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mpogazi/athena_coder/blob/main/video_%26%26_sound.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Video and Sound GPT (Generation of Videos)\n",
        "The main idea in this notebook is to use the gpt model architecture or another different architecture (JEPA for example) to produce plausible videos.\n",
        "\n",
        "### Theory/Intuition:\n",
        "I think it's possible to learn the pixel distributions of the in the time dimension and predict the next frame in videos and other modalities as it has been done for texts. (GPT 3 or GPT 4)\n",
        "\n",
        "### Initial Approach\n",
        "* download hundreds of videos from YouTube and put them on Google drive\n",
        "* Write code to read the videos and transform them into Tensors\n",
        "* Write code to transform or read an output tensors into video format\n",
        "* Write a window code to play the generated video vis as vis the original video (A bit tricky, will add more details as time goes.)"
      ],
      "metadata": {
        "id": "4GRWwksKXXsV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gdk-xH4Yfz-5",
        "outputId": "e562ea89-0dcc-4d27-b351-e7b28547d367"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (0.25.1)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.23.5)\n"
          ]
        }
      ],
      "source": [
        "# Installing the python libraries to handle reading sound and video\n",
        "!pip install opencv-python pydub"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import\n",
        "\n",
        "Please put all the imports here. We would like to have a single source of truth."
      ],
      "metadata": {
        "id": "ABx-XBVxD6fu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from pydub import AudioSegment\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import io\n",
        "import os\n",
        "\n",
        "# Google drive imports\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "pUeR8aAohh8a"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Globals\n",
        "In this section we mount the memory (Drive) and set up some global variables.\n",
        "List of globals:\n",
        "\n",
        "`base_path`, `MAX_HEIGHT`, `MAX_WIDTH`"
      ],
      "metadata": {
        "id": "Z9W-6o-uC5t6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting the drive with the content\n",
        "# Might need to give permission on this.\n",
        "# Since someone needs to access the contents of the drive/videos\n",
        "drive.mount('/content/drive')\n",
        "BASE_PATH = '/content/drive/MyDrive/video_model/'\n",
        "MAX_HEIGHT = 720\n",
        "MAX_WIDTH = 1280\n",
        "TRAIN_SPLIT = 0.9\n",
        "# number of frames in a transformer block\n",
        "BLOCK_SIZE = 3\n",
        "BATCH_SIZE = 4\n",
        "VOCAB_SIZE = 256\n",
        "EMBEDDING_DIM = 16\n",
        "LEARNING_RATE = 3e-4\n",
        "EVAL_ITERS = 20\n",
        "MAX_ITERS = 200"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6I6pX8xhpvy",
        "outputId": "cf9ad17f-c0a1-4d7b-c72e-fbb80c1cacd6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting Up The Training Data\n",
        "\n",
        "1. Create a list of all files\n",
        "2. load the videos to set up some Global Variables"
      ],
      "metadata": {
        "id": "ash84Lf2DniT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files = set()\n",
        "for filename in os.listdir(BASE_PATH):\n",
        "  files.add(filename)\n",
        "\n",
        "def split_test(files):\n",
        "  size = int(len(files) * TRAIN_SPLIT)\n",
        "  input_list = list(files)\n",
        "  random.shuffle(input_list)\n",
        "  return input_list[:size], input_list[size:]\n",
        "\n",
        "train, val = split_test(files)"
      ],
      "metadata": {
        "id": "VLMRPbhBzssV"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Training Data\n",
        "\n",
        "1. Load the videos and pad them\n",
        "2. Collapse all the videos into a single giant data object\n",
        "3. Split the vidoes into train and test data"
      ],
      "metadata": {
        "id": "NXzHXkXyEHuX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We will need to pad the tensors to handle MAX_WIDTH and MAX_HEIGHT\n",
        "# Frames are gonna be the same\n",
        "def pad_batch(batch):\n",
        "  # (batch, block, height, width, chanel)\n",
        "  _, _, height, width, _ = tf.shape(batch).numpy()\n",
        "  paddings = tf.constant([\n",
        "      [0, 0],\n",
        "      [0, 0],\n",
        "      [0, MAX_HEIGHT - height],\n",
        "      [0, MAX_WIDTH - width],\n",
        "      [0, 0]\n",
        "  ])\n",
        "\n",
        "  return tf.pad(batch, paddings, \"CONSTANT\", constant_values=0)\n",
        "\n",
        "# take a video path and return an equivalent tensor\n",
        "# Returns tensor of shape (Frames, MAX_HEIGHT, MAX_WIDTH, 3)\n",
        "def capture_frames_randomly(video_path, blocks = 16, batch_size = 32):\n",
        "  cap = cv2.VideoCapture(video_path)\n",
        "  video_tensors = [[] for i in range(batch_size)]\n",
        "\n",
        "  block_starts = tf.random.uniform(\n",
        "                            (batch_size,),\n",
        "                            maxval= int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) - blocks - 1,\n",
        "                            dtype=tf.int32\n",
        "                          )\n",
        "\n",
        "  for index, block_start in enumerate(block_starts.numpy()):\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, block_start)\n",
        "    for i in range(block_start, block_start + blocks + 1):\n",
        "      ret, frame = cap.read()\n",
        "      if not ret:\n",
        "        print(\"Error: Failed to grab frame.\")\n",
        "      video_tensors[index].append(tf.convert_to_tensor(frame, dtype=tf.int16))\n",
        "\n",
        "  cap.release()\n",
        "  video_tensors = tf.stack([tf.convert_to_tensor(video_tensor) for video_tensor in video_tensors])\n",
        "  return video_tensors"
      ],
      "metadata": {
        "id": "yyU9l3qQh5Iu"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Batching\n",
        "Since the video sizes are humongous, we need to implement a\n",
        "batching strategy that is not conventional.\n",
        "\n",
        "We will be picking a file by random and randomly pick a batch.\n",
        "\n",
        "Some Math:\n",
        "We 're working in milli seconds. Therefore, we will be picking\n",
        "this much time in a video randomly (batch_size * (1 / 25) fps)\n",
        "\n",
        "batch_size is counted in frames.\n"
      ],
      "metadata": {
        "id": "0wGh3JO5DeHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pool_images(batch):\n",
        "  pooling = keras.layers.AveragePooling2D(pool_size=(2, 2), padding='valid')\n",
        "  B, T, H, W, C = batch.shape\n",
        "  batch = tf.reshape(tf.cast(batch, dtype=tf.float32), [B * T, H, W, C])\n",
        "  batch = pooling(batch)\n",
        "  batch = pooling(batch)\n",
        "  _, new_H, new_W, _ = batch.shape\n",
        "  batch = tf.reshape(batch, [B, T, new_H, new_W, C])\n",
        "  return batch\n",
        "\n",
        "def get_batch(split: str):\n",
        "  data = train if split == 'train' else val\n",
        "  # a batch of files\n",
        "  file_index = random.randint(0, len(data) - 1)\n",
        "\n",
        "  # batch => (BATCH, BLOCK + 1, H, W, C)\n",
        "  batch = capture_frames_randomly(BASE_PATH + data[file_index], BLOCK_SIZE, BATCH_SIZE)\n",
        "  batch = pad_batch(batch)\n",
        "  xb, yb = batch[:, :BLOCK_SIZE,:, :, :], batch[:, 1:(BLOCK_SIZE + 1), :, :, :]\n",
        "\n",
        "  return pool_images(xb), pool_images(yb)\n",
        "\n",
        "xb, yb = get_batch('train')"
      ],
      "metadata": {
        "id": "KUh7KSWWlgY2"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Always very important to check the size of the tensors to make sure you're doing the right thing!\"\"\"\n",
        "print(\"shape xb: \", tf.shape(xb))\n",
        "print(\"shape yb: \", tf.shape(yb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6viKq6h3Lzb0",
        "outputId": "d4bc1549-e12d-4211-fd95-20579ea35175"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape xb:  tf.Tensor([  4   3 180 320   3], shape=(5,), dtype=int32)\n",
            "shape yb:  tf.Tensor([  4   3 180 320   3], shape=(5,), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modeling\n",
        "\n",
        "This is the modeling part of the notebook. Watch as we create the model lmaooo!\n",
        "\n",
        "Since me and my associates are GPU-poor, the model should have checkpoints from the beginning. So anytime the GPU dies, after Google and Associates kill the session, we will restart where we were in the training."
      ],
      "metadata": {
        "id": "G07F_rbZQf-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_filepath = '/content/drive/MyDrive/model_checkpoints/video_foundation_model_checkpoint.h5' # path to save weights"
      ],
      "metadata": {
        "id": "E21I6734GeUg"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the ModelCheckpoint callback\n",
        "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',  # You can choose the metric to monitor\n",
        "    mode='max',\n",
        "    save_best_only=True\n",
        ")"
      ],
      "metadata": {
        "id": "y7BE4M5eQpnu"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Model\n",
        "\n",
        "Now let's talk about the structure of the model."
      ],
      "metadata": {
        "id": "69NWX2_PDfZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageDense(layers.Layer):\n",
        "  \"\"\"\n",
        "  This is an attempt to define a dense layer for images.\n",
        "  \"\"\"\n",
        "  def __init__(self, embedding_dim = EMBEDDING_DIM):\n",
        "    super().__init__()\n",
        "\n",
        "  def call(self, x):\n",
        "    B, T, H, W, C, E = x.shape\n",
        "    return x\n",
        "\n",
        "class MultiHeadAttention(layers.Layer):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.c_attention = layers.Dense(EMBEDDING_DIM * 3)\n",
        "\n",
        "\n",
        "  def call(self, x):\n",
        "    B, T, H, W, C, E = x.shape\n",
        "    k, q, v = self.c_attention(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "# class FeedForward(layers.Layer):\n",
        "#   def __init__(self):\n",
        "#     super().__init__()\n",
        "\n",
        "#   def call(self, x):\n",
        "\n",
        "# class Block(layers.Layer):\n",
        "#   def __init__(self):\n",
        "#     super().__init__()\n",
        "\n",
        "#   def call(self, x):"
      ],
      "metadata": {
        "id": "qFl-vUqUVvBr"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VideoModel(keras.Model):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.pixel_embedding_table = layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM, dtype=tf.float32)\n",
        "    self.image_positional_embedding_table = layers.Embedding(BLOCK_SIZE, EMBEDDING_DIM)\n",
        "\n",
        "    self.vm_head = keras.layers.Dense(VOCAB_SIZE)\n",
        "    self.loss_calc = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "  def call(self, idx, targets=None):\n",
        "    B, T, H, W, C = idx.shape\n",
        "    pixel_embedding = self.pixel_embedding_table(idx)\n",
        "    pos_embedding = tf.reshape(self.image_positional_embedding_table(tf.range(T)), [1,T, 1, 1, 1, EMBEDDING_DIM])\n",
        "    x = pixel_embedding + pos_embedding\n",
        "\n",
        "    logits = self.vm_head(x)\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      loss = self.loss_calc(targets, logits)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_images):\n",
        "    B, T, H, W, C = idx.shape\n",
        "    idx = tf.cast(idx, dtype=tf.int32)\n",
        "\n",
        "    for _ in range(max_new_images):\n",
        "      idx_cond = idx[:, -BLOCK_SIZE:]\n",
        "\n",
        "      #print(\"idx_cond: \", tf.shape(idx_cond))\n",
        "      logits, _ = self(idx_cond)\n",
        "\n",
        "      #print(\"idx_logits: \", tf.shape(logits))\n",
        "      # (B, T, H, W, C, VOCAB_SIZE)\n",
        "      logits = logits[:, -1, :, :, :, :]\n",
        "      #print(\"idx_logits2: \", tf.shape(logits))\n",
        "\n",
        "      probs = tf.reshape(tf.nn.softmax(logits), [H * W * C, VOCAB_SIZE])\n",
        "      img_next = tf.reshape(tf.random.categorical(probs, 1), [1, -1, H, W, C])\n",
        "\n",
        "      #print(\"image_next: \", tf.shape(img_next))\n",
        "      img_next = tf.cast(img_next , dtype=tf.int32)\n",
        "      idx = tf.concat([idx, img_next], 1)\n",
        "    return idx\n",
        "\n",
        "m = VideoModel()\n",
        "out, loss = m(xb, yb)\n",
        "optimizer = keras.optimizers.AdamW(learning_rate = LEARNING_RATE)"
      ],
      "metadata": {
        "id": "Cpy36D7ZDcXx"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rfruw0kAgiwS",
        "outputId": "57659bd6-f355-492a-8e7e-8c9885cbd60a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"video_model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_4 (Embedding)     multiple                  4096      \n",
            "                                                                 \n",
            " embedding_5 (Embedding)     multiple                  48        \n",
            "                                                                 \n",
            " dense_2 (Dense)             multiple                  4352      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8496 (33.19 KB)\n",
            "Trainable params: 8496 (33.19 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Step\n",
        "\n",
        "Setting up the training step."
      ],
      "metadata": {
        "id": "jcbkHSW1gogT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(x, y, model, optimizer):\n",
        "  with tf.GradientTape() as tape:\n",
        "    logits, loss = model(x, y)\n",
        "\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  #print(gradients)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "  return loss\n",
        "\n",
        "for iter in range(MAX_ITERS):\n",
        "\n",
        "  if iter % EVAL_ITERS == 0:\n",
        "    out = {}\n",
        "    for split in ['train', 'val']:\n",
        "      losses = [None] * EVAL_ITERS\n",
        "      for k in range(EVAL_ITERS):\n",
        "        x, y = get_batch(split)\n",
        "        logits, loss = m(x, y)\n",
        "        losses [k] = loss.numpy()\n",
        "      mean_loss = tf.reduce_mean(losses)\n",
        "      out[split] = mean_loss.numpy()\n",
        "\n",
        "    print(f\"step {iter}: train loss {out['train']:.4f}, val loss {out['val']:.4f}\")\n",
        "\n",
        "  x, y = get_batch('train')\n",
        "  loss = train_step(x, y, m, optimizer)\n",
        "\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GZjOZ-CgnMc",
        "outputId": "595bc36c-b40a-41a4-9ce1-4259fb51ba61"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 5.5465, val loss 5.5457\n",
            "step 20: train loss 5.5398, val loss 5.5391\n",
            "step 40: train loss 5.5348, val loss 5.5365\n",
            "step 60: train loss 5.5259, val loss 5.5285\n",
            "step 80: train loss 5.5229, val loss 5.5244\n",
            "step 100: train loss 5.5091, val loss 5.5202\n",
            "step 120: train loss 5.5123, val loss 5.5147\n",
            "step 140: train loss 5.4993, val loss 5.5048\n",
            "step 160: train loss 5.4873, val loss 5.4975\n",
            "step 180: train loss 5.4659, val loss 5.4827\n",
            "tf.Tensor(5.503126, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization\n",
        "\n",
        "We need to be able to turn the generated artifacts into videos that are viewable and critiquable. Therefore we need to have the facilities to change a tensor of shape:\n",
        "\n",
        "`[FRAMES, HEIGHT, WIDTH, C]` to a video (normally a 25fps video)."
      ],
      "metadata": {
        "id": "x4om1IPqUcRd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample = m.generate(tf.zeros((1, 1, MAX_HEIGHT // 4, MAX_WIDTH // 4, 3)), 1000)\n",
        "tf.shape(sample)"
      ],
      "metadata": {
        "id": "9bi3u_13UsdT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4f03cbe-54f4-455f-dd5b-4654ab058e43"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5,), dtype=int32, numpy=array([   1, 1001,  180,  320,    3], dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.shape(sample[0][0]).numpy()[:2][::-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnUamtCS5afS",
        "outputId": "bb7f764a-1cc5-4483-e3e0-cbcbf2a32872"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([320, 180], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_file = \"/content/drive/MyDrive/generated_videos/video_\" + str(random.randint(0, 2000)) + \".mp4\"\n",
        "print(\"output file: \", output_file)\n",
        "fps = 30\n",
        "print(\"sample: \", tf.shape(sample))\n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "T, H, W, C = sample.shape[1:]\n",
        "\n",
        "out = cv2.VideoWriter(output_file, fourcc, fps, (W, H))\n",
        "\n",
        "for t in range(T):\n",
        "  frame = sample[0, t]\n",
        "  frame = np.uint8(frame)\n",
        "  out.write(frame)\n",
        "\n",
        "out.release()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeEoQLJBQBmX",
        "outputId": "5bfe70a8-9a6f-4db3-8d9b-35e254fdc861"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output file:  /content/drive/MyDrive/generated_videos/video_1594.mp4\n",
            "sample:  tf.Tensor([   1 1001  180  320    3], shape=(5,), dtype=int32)\n"
          ]
        }
      ]
    }
  ]
}