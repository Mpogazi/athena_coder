{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mpogazi/athena_coder/blob/main/video_%26%26_sound.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Video and Sound GPT (Generation of Videos)\n",
        "The main idea in this notebook is to use the gpt model architecture or another different architecture (JEPA for example) to produce plausible videos.\n",
        "\n",
        "## Theory/Intuition:\n",
        "I think it's possible to learn the pixel distributions of the in the time dimension and predict the next frame in videos and other modalities as it has been done for texts. (GPT 3 or GPT 4)\n",
        "\n",
        "## Initial Approach\n",
        "* download hundreds of videos from YouTube and put them on Google drive\n",
        "* Write code to read the videos and transform them into Tensors\n",
        "* Write code to transform or read an output tensors into video format\n",
        "* Write a window code to play the generated video vis as vis the original video (A bit tricky, will add more details as time goes.)"
      ],
      "metadata": {
        "id": "4GRWwksKXXsV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gdk-xH4Yfz-5",
        "outputId": "4e6ac128-2610-408a-dfb5-53369b7c2982"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.23.5)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n"
          ]
        }
      ],
      "source": [
        "# Installing the python libraries to handle reading sound and video\n",
        "!pip install opencv-python pydub"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import\n",
        "\n",
        "Please put all the imports here. We would like to have a single source of truth."
      ],
      "metadata": {
        "id": "ABx-XBVxD6fu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from pydub import AudioSegment\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import io\n",
        "import os\n",
        "\n",
        "# Google drive imports\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "pUeR8aAohh8a"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Globals\n",
        "In this section we mount the memory (Drive) and set up some global variables.\n",
        "List of globals:\n",
        "\n",
        "`base_path`, `MAX_HEIGHT`, `MAX_WIDTH`"
      ],
      "metadata": {
        "id": "Z9W-6o-uC5t6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting the drive with the content\n",
        "# Might need to give permission on this.\n",
        "# Since someone needs to access the contents of the drive/videos\n",
        "drive.mount('/content/drive')\n",
        "BASE_PATH = '/content/drive/MyDrive/video_model/'\n",
        "MAX_HEIGHT = 720\n",
        "MAX_WIDTH = 1280\n",
        "TRAIN_SPLIT = 0.9\n",
        "# number of frames in a transformer block\n",
        "BLOCK_SIZE = 3\n",
        "BATCH_SIZE = 1\n",
        "VOCAB_SIZE = 256\n",
        "EMBEDDING_DIM = 16\n",
        "LEARNING_RATE = 3e-4\n",
        "EVAL_ITERS = 20\n",
        "MAX_ITERS = 200"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6I6pX8xhpvy",
        "outputId": "41222952-57a8-4f09-b3c5-b66e5ab07645"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Up The Training Data\n",
        "\n",
        "1. Create a list of all files\n",
        "2. load the videos to set up some Global Variables"
      ],
      "metadata": {
        "id": "ash84Lf2DniT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files = set()\n",
        "for filename in os.listdir(BASE_PATH):\n",
        "  files.add(filename)\n",
        "\n",
        "def split_test(files):\n",
        "  size = int(len(files) * TRAIN_SPLIT)\n",
        "  input_list = list(files)\n",
        "  random.shuffle(input_list)\n",
        "  return input_list[:size], input_list[size:]\n",
        "\n",
        "train, val = split_test(files)"
      ],
      "metadata": {
        "id": "VLMRPbhBzssV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Training Data\n",
        "\n",
        "1. Load the videos and pad them\n",
        "2. Collapse all the videos into a single giant data object\n",
        "3. Split the vidoes into train and test data"
      ],
      "metadata": {
        "id": "NXzHXkXyEHuX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We will need to pad the tensors to handle MAX_WIDTH and MAX_HEIGHT\n",
        "# Frames are gonna be the same\n",
        "def pad_batch(batch):\n",
        "  # (batch, block, height, width, chanel)\n",
        "  _, _, height, width, _ = tf.shape(batch).numpy()\n",
        "  paddings = tf.constant([\n",
        "      [0, 0],\n",
        "      [0, 0],\n",
        "      [0, MAX_HEIGHT - height],\n",
        "      [0, MAX_WIDTH - width],\n",
        "      [0, 0]\n",
        "  ])\n",
        "\n",
        "  return tf.pad(batch, paddings, \"CONSTANT\", constant_values=0)\n",
        "\n",
        "# take a video path and return an equivalent tensor\n",
        "# Returns tensor of shape (Frames, MAX_HEIGHT, MAX_WIDTH, 3)\n",
        "def capture_frames_randomly(video_path, blocks = 16, batch_size = 32):\n",
        "  cap = cv2.VideoCapture(video_path)\n",
        "  video_tensors = [[] for i in range(batch_size)]\n",
        "\n",
        "  block_starts = tf.random.uniform(\n",
        "                            (batch_size,),\n",
        "                            maxval= int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) - blocks - 1,\n",
        "                            dtype=tf.int32\n",
        "                          )\n",
        "\n",
        "  for index, block_start in enumerate(block_starts.numpy()):\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, block_start)\n",
        "    for i in range(block_start, block_start + blocks + 1):\n",
        "      ret, frame = cap.read()\n",
        "      if not ret:\n",
        "        print(\"Error: Failed to grab frame.\")\n",
        "      video_tensors[index].append(tf.convert_to_tensor(frame))\n",
        "\n",
        "  cap.release()\n",
        "  video_tensors = tf.stack([tf.convert_to_tensor(video_tensor) for video_tensor in video_tensors])\n",
        "  return video_tensors"
      ],
      "metadata": {
        "id": "yyU9l3qQh5Iu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Batching\n",
        "Since the video sizes are humongous, we need to implement a\n",
        "batching strategy that is not conventional.\n",
        "\n",
        "We will be picking a file by random and randomly pick a batch.\n",
        "\n",
        "Some Math:\n",
        "We 're working in milli seconds. Therefore, we will be picking\n",
        "this much time in a video randomly (batch_size * (1 / 25) fps)\n",
        "\n",
        "batch_size is counted in frames.\n"
      ],
      "metadata": {
        "id": "0wGh3JO5DeHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split: str):\n",
        "  data = train if split == 'train' else val\n",
        "  # a batch of files\n",
        "  file_index = random.randint(0, len(data) - 1)\n",
        "\n",
        "  # batch => (BATCH, BLOCK + 1, H, W, C)\n",
        "  batch = capture_frames_randomly(BASE_PATH + data[file_index], BLOCK_SIZE, BATCH_SIZE)\n",
        "  print(\"shape: \", tf.shape(batch))\n",
        "  batch = pad_batch(batch)\n",
        "  xb, yb = batch[:, :BLOCK_SIZE,:, :, :], batch[:, 1:(BLOCK_SIZE + 1), :, :, :]\n",
        "\n",
        "  return xb, yb\n",
        "\n",
        "xb, yb = get_batch('train')"
      ],
      "metadata": {
        "id": "KUh7KSWWlgY2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c802f3e1-0b59-47a4-cfec-ebd8277f4541"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape:  tf.Tensor([   1    4  720 1280    3], shape=(5,), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Always very important to check the size of the tensors to make sure you're doing the right thing!\"\"\"\n",
        "print(\"shape xb: \", tf.shape(xb))\n",
        "print(\"shape yb: \", tf.shape(yb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6viKq6h3Lzb0",
        "outputId": "87cf5ae6-249b-476a-eda9-b7a16e60fddb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape xb:  tf.Tensor([   1    3  720 1280    3], shape=(5,), dtype=int32)\n",
            "shape yb:  tf.Tensor([   1    3  720 1280    3], shape=(5,), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checkpoints\n",
        "\n",
        "This is the modeling part of the notebook. Watch as we create the model lmaooo!\n",
        "\n",
        "Since me and my associates are GPU-poor, the model should have checkpoints from the beginning. So anytime the GPU dies, after Google and Associates kill the session, we will restart where we were in the training."
      ],
      "metadata": {
        "id": "G07F_rbZQf-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_filepath = '/content/drive/MyDrive/model_checkpoints/video_foundation_model_checkpoint.h5' # path to save weights"
      ],
      "metadata": {
        "id": "E21I6734GeUg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the ModelCheckpoint callback\n",
        "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',  # You can choose the metric to monitor\n",
        "    mode='max',\n",
        "    save_best_only=True\n",
        ")"
      ],
      "metadata": {
        "id": "y7BE4M5eQpnu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experimentation\n",
        "\n",
        "Run all the sample code you're about to add to the model here. It's a good practice to write the code down and experiment on small randomly generated data to check the dimensions matching, accuracy of the operations, and really how to compose them in cool ways."
      ],
      "metadata": {
        "id": "oi5S_8zsxGdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "B, T, C = [2, 4, 32]\n",
        "sample = tf.random.normal([B, T, C])\n",
        "## 45, 80\n",
        "sample = layers.Dense(45 * 80 * 3)(sample)\n",
        "print(\"shape1: \", tf.shape(sample))\n",
        "sample = tf.reshape(sample, [B * T, 45, 80, 3])\n",
        "print(\"shape2: \", tf.shape(sample))\n",
        "sample = keras.layers.Conv2DTranspose(16, 2, 2, activation='relu')(sample)\n",
        "print(\"shape3: \", tf.shape(sample))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLbiPQ35egEz",
        "outputId": "4e458f7c-9e02-4a1c-c6e0-03ff4ebfbbbf"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape1:  tf.Tensor([    2     4 10800], shape=(3,), dtype=int32)\n",
            "shape2:  tf.Tensor([ 8 45 80  3], shape=(4,), dtype=int32)\n",
            "shape3:  tf.Tensor([  8  90 160  16], shape=(4,), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Model\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "69NWX2_PDfZ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Layers\n",
        "\n",
        "Use this part of the notebook to describe the layers being used in the model."
      ],
      "metadata": {
        "id": "tzMcVOJex-Gr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageEncoder(layers.Layer):\n",
        "  def __init__(self, filter_size = 32, kernel_size=3, encoding_dim = 32):\n",
        "    super().__init__()\n",
        "    ## The Encoder is gonna be some kind of convolutional neural network\n",
        "    self.conv1 = layers.Conv2D(filter_size, kernel_size, padding=\"same\", strides=1)\n",
        "    self.pool1 = layers.MaxPooling2D(pool_size=(3, 3), padding=\"same\", strides=2)\n",
        "\n",
        "    self.conv2 = layers.Conv2D(filter_size, kernel_size, padding=\"same\", strides=1)\n",
        "    self.pool2 = layers.MaxPooling2D(pool_size=(3, 3), padding=\"same\", strides=2)\n",
        "\n",
        "    self.conv3 = layers.Conv2D(filter_size, kernel_size, padding=\"same\", strides=1)\n",
        "    self.pool3 = layers.MaxPooling2D(pool_size=(3, 3), padding=\"same\", strides=2)\n",
        "\n",
        "    self.conv4 = layers.Conv2D(filter_size, kernel_size, padding=\"same\", strides=1)\n",
        "    self.pool4 = layers.MaxPooling2D(pool_size=(3, 3), padding=\"same\", strides=2)\n",
        "\n",
        "    self.conv5 = layers.Conv2D(filter_size, kernel_size, padding=\"same\", strides=1)\n",
        "    self.pool5 = layers.MaxPooling2D(pool_size=(3, 3), padding=\"same\", strides=2)\n",
        "\n",
        "    self.conv6 = layers.Conv2D(filter_size, kernel_size, padding=\"same\", strides=1)\n",
        "    self.pool6 = layers.MaxPooling2D(pool_size=(3, 3), padding=\"same\", strides=2)\n",
        "\n",
        "    self.flatten = layers.Flatten()\n",
        "    self.dense = layers.Dense(encoding_dim, activation='relu')\n",
        "\n",
        "  def call(self, x):\n",
        "    # x is of shape (B, T, H, W, C)\n",
        "    # reshape it to (B * T, H, W, C) to perform the convolutions\n",
        "    B, T, H, W, C = x.shape\n",
        "    x = tf.reshape(x, [B * T, H, W, C])\n",
        "\n",
        "    # conv block 1\n",
        "    x = self.pool1(self.conv1(x))\n",
        "    # conv block 2\n",
        "    x = self.pool2(self.conv2(x))\n",
        "    # conv block 3\n",
        "    x = self.pool3(self.conv3(x))\n",
        "    # conv block 4\n",
        "    x = self.pool4(self.conv4(x))\n",
        "    # conv block 5\n",
        "    x = self.pool5(self.conv5(x))\n",
        "    # conv block 6\n",
        "    x = self.pool6(self.conv6(x))\n",
        "\n",
        "    # with now a tensor of shape (B * T, feature_height, feature_width, filter_size)\n",
        "    # Let's flatten it to (B * T, feature_height * feature_width * filter_size)\n",
        "    x = self.flatten(x)\n",
        "\n",
        "    # Let's now apply a dense layer to bring back the images to tokens of size 256 (any other units count is also acceptable)\n",
        "    # output tensor is (B * T, 256)\n",
        "    x = self.dense(x)\n",
        "\n",
        "    # Now back to batch and time dimensions\n",
        "    # output tensor (B, T, 256) --[-1] means that dimension is computed.\n",
        "    x = tf.reshape(x, [B, T, -1])\n",
        "\n",
        "    # Now we're ready to feed this to a transform\n",
        "    return x\n",
        "\n",
        "class ImageDecoder(layers.Layer):\n",
        "  def __init__(self, filter_size = 32, kernel_size=3, encoding_dim = 32):\n",
        "    super().__init__()\n",
        "    ## The decoder is gonna be some kind of transposed convolutional neural network\n",
        "    self.filter_size = filter_size\n",
        "\n",
        "    self.dense1 = layers.Dense(filter_size * 16 * 16, activation='relu')\n",
        "    self.upconv1 = layers.Conv2DTranspose(filter_size, kernel_size, padding=\"same\", strides=2)\n",
        "\n",
        "    self.dense2 = layers.Dense(filter_size * 32 * 32, activation='relu')\n",
        "    self.upconv2 = layers.Conv2DTranspose(filter_size, kernel_size, padding=\"same\", strides=2)\n",
        "\n",
        "    self.dense3 = layers.Dense(filter_size * 64 * 64, activation='relu')\n",
        "    self.upconv3 = layers.Conv2DTranspose(filter_size, kernel_size, padding=\"same\", strides=2)\n",
        "\n",
        "    self.dense4 = layers.Dense(filter_size * 128 * 128, activation='relu')\n",
        "    self.upconv4 = layers.Conv2DTranspose(filter_size, kernel_size, padding=\"same\", strides=2)\n",
        "\n",
        "    self.conv1 = layers.Conv2D(3, kernel_size, padding=\"same\", activation='relu')\n",
        "\n",
        "  def call(self, x):\n",
        "    # x is of shape (B, T, 256)\n",
        "    # let's apply a dense layer to bring it back to a shape suitable for deconvolution\n",
        "    x = self.dense1(x)\n",
        "    # output tensor is (B, T, 16 * 16 * 128)\n",
        "\n",
        "    # reshape it to (B * T, 16, 16, 128) to perform the transposed convolutions\n",
        "    B, T, _ = x.shape\n",
        "    x = tf.reshape(x, [B * T, 16, 16, self.filter_size])\n",
        "\n",
        "    # deconv block 4\n",
        "    x = self.upconv1(x)\n",
        "\n",
        "    # deconv block 3\n",
        "    x = tf.reshape(self.dense2(x), [B * T, 32, 32, self.filter_size])\n",
        "    x = self.upconv2(x)\n",
        "\n",
        "    # deconv block 2\n",
        "    x = tf.reshape(self.dense3(x), [B * T, 64, 64, self.filter_size])\n",
        "    x = self.upconv3(x)\n",
        "\n",
        "    # deconv block 1\n",
        "    x = tf.reshape(self.dense4(x), [B * T, 128, 128, self.filter_size])\n",
        "    x = self.upconv4(x)\n",
        "\n",
        "    # finally, apply a conv layer to get the final image\n",
        "    # output tensor is (B * T, H, W, C)\n",
        "    x = self.conv1(x)\n",
        "\n",
        "    # reshape it back to (B, T, H, W, C)\n",
        "    x = tf.reshape(x, [B, T, *x.shape[-3:]])\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "qFl-vUqUVvBr"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Definition\n",
        "\n",
        "Use this place to define the model."
      ],
      "metadata": {
        "id": "ux3GQLW1xxAb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VideoModel(keras.Model):\n",
        "  def __init__(self, encoding_dim = 32):\n",
        "    super().__init__()\n",
        "    # encoder\n",
        "    self.image_encoder =  ImageEncoder()\n",
        "    # predictor\n",
        "    self.predictor = layers.Dense(encoding_dim, activation='relu')\n",
        "    # decoder\n",
        "    self.image_decoder =  ImageDecoder()\n",
        "    # loss calculator\n",
        "    self.loss_calc = keras.losses.MeanSquaredError()\n",
        "\n",
        "  def preprocess_images(self, images):\n",
        "    images = tf.cast(images, dtype=tf.float32)\n",
        "    return images / 255.0\n",
        "\n",
        "  def call(self, images, targets=None):\n",
        "    B, T, H, W, C = images.shape\n",
        "\n",
        "    # Encoding\n",
        "    x = self.preprocess_images(images)\n",
        "\n",
        "    s_x = self.image_encoder(x)\n",
        "    tf.print(tf.shape(s_x))\n",
        "\n",
        "    # Prediction in the Latent space\n",
        "    s_tilde_y = self.predictor(s_x)\n",
        "    tf.print(tf.shape(s_tilde_y))\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      y   = self.preprocess_images(targets)\n",
        "      s_y = self.image_encoder(y)\n",
        "      loss = self.loss_calc(s_tilde_y, s_y)\n",
        "\n",
        "    y_pred = self.image_decoder(x)\n",
        "    # y => [B, T, H, W, C]\n",
        "    return y_pred, loss\n",
        "\n",
        "  def generate(self, idx, max_new_images):\n",
        "    B, T, H, W, C = idx.shape\n",
        "    idx = tf.cast(idx, dtype=tf.int32)\n",
        "\n",
        "    for _ in range(max_new_images):\n",
        "      idx_cond = idx[:, -BLOCK_SIZE:]\n",
        "\n",
        "      #print(\"idx_cond: \", tf.shape(idx_cond))\n",
        "      images, _ = self(idx_cond)\n",
        "\n",
        "      #print(\"idx_logits: \", tf.shape(logits))\n",
        "      # [B, T, H, W, C]\n",
        "      img_next = logits[:, -1,:,:,:]\n",
        "      img_next = tf.cast(img_next , dtype=tf.int32)\n",
        "      idx = tf.concat([idx, img_next], 1)\n",
        "    return idx\n",
        "\n",
        "m = VideoModel()\n",
        "out, loss = m(xb, yb)\n",
        "optimizer = keras.optimizers.AdamW(learning_rate = LEARNING_RATE)"
      ],
      "metadata": {
        "id": "Cpy36D7ZDcXx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "outputId": "408fdbcb-1005-41a7-ff81-7130c253e95f"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 3 32]\n",
            "[1 3 32]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-93f8dfda18d4>\u001b[0m in \u001b[0;36m<cell line: 59>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVideoModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-52-93f8dfda18d4>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     35\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_calc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_tilde_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0;31m# y => [B, T, H, W, C]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-51-7dfa558765bb>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# x is of shape (B, T, 256)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;31m# let's apply a dense layer to bring it back to a shape suitable for deconvolution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;31m# output tensor is (B, T, 16 * 16 * 128)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: Exception encountered when calling layer 'dense_74' (type Dense).\n\n{{function_node __wrapped__MatMul_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[2764800,8192] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:MatMul] name: \n\nCall arguments received by layer 'dense_74' (type Dense):\n  • inputs=tf.Tensor(shape=(1, 3, 720, 1280, 3), dtype=float32)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rfruw0kAgiwS",
        "outputId": "57659bd6-f355-492a-8e7e-8c9885cbd60a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"video_model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_4 (Embedding)     multiple                  4096      \n",
            "                                                                 \n",
            " embedding_5 (Embedding)     multiple                  48        \n",
            "                                                                 \n",
            " dense_2 (Dense)             multiple                  4352      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8496 (33.19 KB)\n",
            "Trainable params: 8496 (33.19 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training\n",
        "\n",
        "Setting up the training step."
      ],
      "metadata": {
        "id": "jcbkHSW1gogT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(x, y, model, optimizer):\n",
        "  with tf.GradientTape() as tape:\n",
        "    logits, loss = model(x, y)\n",
        "\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  #print(gradients)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "  return loss\n",
        "\n",
        "for iter in range(MAX_ITERS):\n",
        "\n",
        "  if iter % EVAL_ITERS == 0:\n",
        "    out = {}\n",
        "    for split in ['train', 'val']:\n",
        "      losses = [None] * EVAL_ITERS\n",
        "      for k in range(EVAL_ITERS):\n",
        "        x, y = get_batch(split)\n",
        "        logits, loss = m(x, y)\n",
        "        losses [k] = loss.numpy()\n",
        "      mean_loss = tf.reduce_mean(losses)\n",
        "      out[split] = mean_loss.numpy()\n",
        "\n",
        "    print(f\"step {iter}: train loss {out['train']:.4f}, val loss {out['val']:.4f}\")\n",
        "\n",
        "  x, y = get_batch('train')\n",
        "  loss = train_step(x, y, m, optimizer)\n",
        "\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GZjOZ-CgnMc",
        "outputId": "595bc36c-b40a-41a4-9ce1-4259fb51ba61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 5.5465, val loss 5.5457\n",
            "step 20: train loss 5.5398, val loss 5.5391\n",
            "step 40: train loss 5.5348, val loss 5.5365\n",
            "step 60: train loss 5.5259, val loss 5.5285\n",
            "step 80: train loss 5.5229, val loss 5.5244\n",
            "step 100: train loss 5.5091, val loss 5.5202\n",
            "step 120: train loss 5.5123, val loss 5.5147\n",
            "step 140: train loss 5.4993, val loss 5.5048\n",
            "step 160: train loss 5.4873, val loss 5.4975\n",
            "step 180: train loss 5.4659, val loss 5.4827\n",
            "tf.Tensor(5.503126, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization\n",
        "\n",
        "We need to be able to turn the generated artifacts into videos that are viewable and critiquable. Therefore we need to have the facilities to change a tensor of shape:\n",
        "\n",
        "`[FRAMES, HEIGHT, WIDTH, C]` to a video (normally a 25fps video)."
      ],
      "metadata": {
        "id": "x4om1IPqUcRd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample = m.generate(tf.zeros((1, 1, MAX_HEIGHT // 4, MAX_WIDTH // 4, 3)), 1000)\n",
        "tf.shape(sample)"
      ],
      "metadata": {
        "id": "9bi3u_13UsdT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4f03cbe-54f4-455f-dd5b-4654ab058e43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5,), dtype=int32, numpy=array([   1, 1001,  180,  320,    3], dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.shape(sample[0][0]).numpy()[:2][::-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnUamtCS5afS",
        "outputId": "bb7f764a-1cc5-4483-e3e0-cbcbf2a32872"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([320, 180], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_file = \"/content/drive/MyDrive/generated_videos/video_\" + str(random.randint(0, 2000)) + \".mp4\"\n",
        "print(\"output file: \", output_file)\n",
        "fps = 30\n",
        "print(\"sample: \", tf.shape(sample))\n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "T, H, W, C = sample.shape[1:]\n",
        "\n",
        "out = cv2.VideoWriter(output_file, fourcc, fps, (W, H))\n",
        "\n",
        "for t in range(T):\n",
        "  frame = sample[0, t]\n",
        "  frame = np.uint8(frame)\n",
        "  out.write(frame)\n",
        "\n",
        "out.release()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeEoQLJBQBmX",
        "outputId": "5bfe70a8-9a6f-4db3-8d9b-35e254fdc861"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output file:  /content/drive/MyDrive/generated_videos/video_1594.mp4\n",
            "sample:  tf.Tensor([   1 1001  180  320    3], shape=(5,), dtype=int32)\n"
          ]
        }
      ]
    }
  ]
}